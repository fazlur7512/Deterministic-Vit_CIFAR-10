{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazlur7512/Deterministic-Vit_CIFAR-10/blob/main/ViT_Custom_LN_MHA_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZlaLhCwRJ-S",
        "outputId": "22eaf336-69d8-4e76-c998-3a445e1353da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (23.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVK-xX1xQEi8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    LayerNormalization,\n",
        ")\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxNE_yULbtun"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl6lZSD3RTsS"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = Dense(embed_dim)\n",
        "        self.key_dense = Dense(embed_dim)\n",
        "        self.value_dense =Dense(embed_dim)\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wwhReSzbWit"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(tf.keras.layers.Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=tf.keras.initializers.Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=tf.keras.initializers.Zeros(), trainable=True)\n",
        "        super(LayerNorm, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lrKlsR6RdND"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "                Dense(mlp_dim, activation=tfa.activations.gelu),\n",
        "                Dropout(dropout),\n",
        "                Dense(embed_dim),\n",
        "                Dropout(dropout)])\n",
        "        \n",
        "        self.layernorm1 = LayerNorm(eps=1e-6)\n",
        "        self.layernorm2 = LayerNorm(eps=1e-6)\n",
        "        self.dropout1 = Dropout(dropout)\n",
        "        self.dropout2 = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        inputs_norm = self.layernorm1(inputs)\n",
        "        print(\"shape of input before MHA\",inputs_norm.shape)\n",
        "        attn_output = self.att(inputs_norm)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = attn_output + inputs\n",
        "        print(\"shape after dropout\", out1.shape)\n",
        "\n",
        "        out1_norm = self.layernorm2(out1)\n",
        "        mlp_output = self.mlp(out1_norm)\n",
        "        mlp_output = self.dropout2(mlp_output, training=training)\n",
        "        return mlp_output + out1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IiiqdBIhlKs"
      },
      "outputs": [],
      "source": [
        "class Custom_Dense(keras.layers.Layer):   \n",
        "    def __init__(self, units):\n",
        "        super(Custom_Dense, self).__init__()\n",
        "        self.units = units      \n",
        "                  \n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(name = 'w', shape=(input_shape[1]*input_shape[2]*input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None), \n",
        "            trainable=True,\n",
        "        )          \n",
        "    def call(self, input_in):\n",
        "        batch_size = input_in.shape[0]           \n",
        "        flatt = tf.reshape(input_in, [batch_size, -1]) #shape=[batch_size, im_size*im_size*num_channel]           \n",
        "        out = tf.matmul(flatt, self.w)        \n",
        "        return out  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-l1sSHWRowY"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        mlp_dim,\n",
        "        channels=3,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "\n",
        "        self.rescale = Rescaling(1.0 / 255)\n",
        "        self.pos_emb = self.add_weight(\n",
        "            \"pos_emb\", shape=(1, num_patches + 1, d_model)\n",
        "        )\n",
        "        self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
        "        self.patch_proj = Dense(d_model)\n",
        "        self.enc_layers = [\n",
        "            TransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.mlp_head = tf.keras.Sequential(\n",
        "            [\n",
        "                LayerNorm(eps=1e-6),\n",
        "                Dense(mlp_dim, activation=tfa.activations.gelu),\n",
        "                Dropout(dropout),\n",
        "                Dense(num_classes),\n",
        "            ]\n",
        "        )\n",
        "    def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
        "        return patches\n",
        "\n",
        "    def call(self, x, training):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = self.rescale(x)\n",
        "        patches = self.extract_patches(x)\n",
        "        x = self.patch_proj(patches)\n",
        "\n",
        "        class_emb = tf.broadcast_to(\n",
        "            self.class_emb, [batch_size, 1, self.d_model]\n",
        "        )\n",
        "        x = tf.concat([class_emb, x], axis=1)\n",
        "        x = x + self.pos_emb\n",
        "\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, training)\n",
        "\n",
        "        # First (class token) is used for classification\n",
        "        x = self.mlp_head(x[:, 0])\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXXLLh1oSHcs"
      },
      "outputs": [],
      "source": [
        "model = VisionTransformer(\n",
        "            image_size=32,\n",
        "            patch_size=4,\n",
        "            num_layers=4,\n",
        "            num_classes=10,\n",
        "            d_model=64,\n",
        "            num_heads=4,\n",
        "            mlp_dim=128,\n",
        "            channels=3,\n",
        "            dropout=0.1,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZajZVLDMSLZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bef866-e891-4924-e34c-0d4260baafbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcqbMFbsSO60"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer='adam',\n",
        "            metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3IHTSyVSPqT",
        "outputId": "47fe3ff9-96e8-4e23-e6ed-eabd4ba00d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "1563/1563 [==============================] - ETA: 0s - loss: 1.7601 - accuracy: 0.3369shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "shape of input before MHA (None, 65, 64)\n",
            "shape after dropout (None, 65, 64)\n",
            "1563/1563 [==============================] - 258s 157ms/step - loss: 1.7601 - accuracy: 0.3369 - val_loss: 1.5497 - val_accuracy: 0.4242\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 244s 156ms/step - loss: 1.4876 - accuracy: 0.4566 - val_loss: 1.4317 - val_accuracy: 0.4783\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 245s 157ms/step - loss: 1.3799 - accuracy: 0.5000 - val_loss: 1.3811 - val_accuracy: 0.4990\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 245s 156ms/step - loss: 1.3202 - accuracy: 0.5221 - val_loss: 1.2772 - val_accuracy: 0.5344\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 245s 156ms/step - loss: 1.2682 - accuracy: 0.5420 - val_loss: 1.2564 - val_accuracy: 0.5439\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 244s 156ms/step - loss: 1.2285 - accuracy: 0.5541 - val_loss: 1.2545 - val_accuracy: 0.5502\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 245s 156ms/step - loss: 1.1852 - accuracy: 0.5712 - val_loss: 1.2072 - val_accuracy: 0.5617\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 244s 156ms/step - loss: 1.1580 - accuracy: 0.5801 - val_loss: 1.1902 - val_accuracy: 0.5684\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 243s 155ms/step - loss: 1.1236 - accuracy: 0.5979 - val_loss: 1.1749 - val_accuracy: 0.5848\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 243s 156ms/step - loss: 1.0961 - accuracy: 0.6046 - val_loss: 1.1570 - val_accuracy: 0.5853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ff4e29c10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.fit(trainX,trainY,epochs = 10,validation_data = (testX, testY))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKouO6kq4l1Rgm3tVZgFjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}